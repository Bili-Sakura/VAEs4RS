
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{The Robustness of Natural English Priors in Remote Sensing: \\ A Zero-Shot VAE Study}

\author{Anonymous Authors\\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
This paper explores the robustness of variational autoencoders (VAEs) pre-trained on natural image datasets, such as ImageNet, when applied to the remote sensing domain in a zero-shot manner. We investigate whether these "natural English priors" embedded in standard VAEs can serve as effective compressors and reconstructors for satellite imagery, which often exhibits significantly different statistical properties compared to natural scenes. Our study evaluates several state-of-the-art VAE architectures—including SD21-VAE, SDXL-VAE, SD35-VAE, FLUX.1/2-VAE, SANA-VAE, and Qwen-VAE—across multiple remote sensing categories using standard reconstruction metrics. Furthermore, we demonstrate the potential of zero-shot VAEs as cheap pre-processors for denoising and de-hazing remote sensing data.
\end{abstract}

\section{Introduction}

The field of computer vision has witnessed a dramatic rise in foundation models, particularly generative vision models capable of high-fidelity visual generation. Milestone architectures such as GAN-based models like StyleGAN~\citep{karras2019style} and GigaGAN~\citep{kang2023gigagan} have set early benchmarks in generative modeling. More recently, the field has been dominated by a vast array of diffusion models, including DDPM~\citep{ho2020denoising}, ADM~\citep{dhariwal2021diffusion}, EDM~\citep{karras2022elucidating}, and latent diffusion models (LDM) such as Stable Diffusion~\citep{rombachHighResolutionImageSynthesis2022}. Furthermore, the emergence of large multimodal models~\citep{wangMultimodalLearningNexttoken2026} and vision-language models for remote sensing~\citep{liVisionLanguageModelsRemote2024, wengVisionLanguageModelingMeets2025} has further expanded the scope of foundation models in the field. These models have demonstrated unprecedented strength in capturing complex visual distributions across diverse domains.

In the remote sensing domain, these visual generative models have also come into significant interest, with recent surveys highlighting the potential of vision foundation models~\citep{luVisionFoundationModels2025, xiaoFoundationModelsRemote2025, tuiaArtificialIntelligenceAdvance2025} and multimodal models~\citep{baiOrbitGroundComprehensive2025, liuRemoteSensingSpatiotemporal2025} in Earth observation. A common practice is to employ standard VAEs pre-trained on general domain data, such as ImageNet, without further domain-specific adaptation or fine-tuning. This raises a fundamental question: can these pre-trained VAEs serve as reliable compressors or reconstructors when adopted to an out-of-domain context like remote sensing? Given that the visual priors in these models are primarily derived from "natural" English-centric datasets, their robustness in specialized domains remains an open area of investigation.

The challenges in remote sensing are compounded by the inherent differences between natural images and satellite observations. Unlike natural scenes, remote sensing data often involves unique viewing geometries, multi-spectral bands, and varying spatial resolutions~\citep{chengRemoteSensingImage2017}. Furthermore, labeled datasets in remote sensing are frequently sparse, with variable labeling schemes and qualities~\citep{christieFunctionalMapWorld2018}. In contrast to standard machine learning settings where observations $X$ and labels $Y$ are definitive pairs, satellite machine learning often deals with label annotations generated independently of specific satellite observations. Instead, labels are often paired with many different choices of satellite observations corresponding to the label's location and time index, introducing further complexity into the learning process.

\section{Related Work}

\textbf{Generative Models for Remote Sensing:} Recent work has explored the application of generative models to various remote sensing tasks, including scene classification~\citep{chengRemoteSensingImage2017}, object localization~\citep{longAccurateObjectLocalization2017}, and image retrieval~\citep{xiaoHighResolutionRemoteSensing2017}. The shift towards foundation models has led to the development of specialized architectures for Earth observation~\citep{luVisionFoundationModels2025}.

\textbf{Variational Autoencoders and Latent Spaces:} VAEs~\citep{kingmaAutoEncodingVariationalBayes2014} have long been used for representation learning. Recent advancements such as VQGAN~\citep{esserTamingTransformersHighResolution2021} and its successors have improved the quality of latent spaces for high-resolution synthesis. Newer approaches like REPA-E~\citep{lengREPAEUnlockingVAE2025} and representation autoencoders~\citep{zhengDiffusionTransformersRepresentation2025, tongScalingTexttoImageDiffusion2026} aim to align generative and discriminative representations, which is particularly relevant for zero-shot transfer across domains.

\section{Standard Variational Autoencoders}

Standard Variational Autoencoders (VAEs)~\citep{kingmaAutoEncodingVariationalBayes2014} aim to learn a compressed representation of data by mapping input images to a latent space through an encoder and reconstructing them via a decoder. The optimization objective typically involves a reconstruction loss and a Kullback-Leibler (KL) divergence term to regularize the latent space. In this study, we evaluate several state-of-the-art VAE architectures in a zero-shot manner:

\begin{itemize}
    \item \textbf{SD21-VAE:} The standard VAE used in Stable Diffusion 2.1~\citep{rombachHighResolutionImageSynthesis2022}, known for its robust reconstruction capabilities across various natural image domains.
    \item \textbf{SDXL-VAE:} An improved VAE architecture introduced with Stable Diffusion XL~\citep{podellSDXLImprovingLatent2024}, designed to handle higher-resolution images with fewer artifacts.
    \item \textbf{SD35-VAE:} The VAE component of Stable Diffusion 3.5, which is optimized for the rectified flow transformer architecture~\citep{esserScalingRectifiedFlow2024}.
    \item \textbf{FLUX1-VAE:} The VAE from the FLUX.1 family~\citep{labsFLUX1KontextFlow2025}, which utilizes advanced flow-matching techniques for high-fidelity reconstruction.
    \item \textbf{FLUX2-VAE:} The latest iteration from the FLUX family~\citep{labFLUX2FrontierVisual2025}, further pushing the boundaries of visual intelligence.
    \item \textbf{SANA-VAE:} A highly efficient VAE designed for high-resolution synthesis with linear diffusion transformers~\citep{xieSANAEfficientHighResolution2025}.
    \item \textbf{Qwen-VAE:} The visual encoding component of the Qwen-Image model~\citep{wuQwenImageTechnicalReport2025}, pre-trained on a diverse range of visual and textual data.
\end{itemize}

\section{Experimental Results}

We evaluate the performance of various VAE architectures on two benchmark remote sensing datasets: NWPU-RESISC45~\citep{chengRemoteSensingImage2017} and AID~\citep{xia2017aid}. These datasets cover a wide range of aerial scene categories, providing a comprehensive evaluation of the models' zero-shot capabilities.

\subsection{Datasets}

\textbf{NWPU-RESISC45:} This dataset contains 31,500 images of $256 \times 256$ pixels, divided into 45 scene classes such as airplane, airport, bridge, forest, and wetland. It is characterized by high variability in spatial resolution (20cm to 30m) and environmental conditions.

\textbf{AID:} The Aerial Image Dataset consists of 10,000 images of $600 \times 600$ pixels across 30 categories. The images are extracted from Google Earth and labeled by specialists, representing a diverse set of aerial scenes from around the world.

\subsection{Main Results}

Table~\ref{tab:main_results} summarizes the performance of different VAEs across multiple metrics on the combined test sets of RESISC45 and AID, including PSNR, SSIM~\citep{wangImageQualityAssessment2004}, LPIPS~\citep{zhangUnreasonableEffectivenessDeep2018}, and reconstruction FID~\citep{heuselGANsTrainedTwo2017}.

\begin{table}[h]
\caption{Main Results: Comparison of VAEs on Remote Sensing Reconstruction.}
\label{tab:main_results}
\begin{center}
\begin{tabular}{lccccc}
\bf Dataset & \bf Model & \bf PSNR $\uparrow$ & \bf SSIM $\uparrow$ & \bf LPIPS $\downarrow$ & \bf FID $\downarrow$ \\
\hline \\
RESISC45 & SD21-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
RESISC45 & SDXL-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
RESISC45 & SD35-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
RESISC45 & FLUX1-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
RESISC45 & FLUX2-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
RESISC45 & SANA-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
RESISC45 & Qwen-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
AID & SD21-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
AID & SDXL-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
AID & SD35-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
AID & FLUX1-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
AID & FLUX2-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
AID & SANA-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
AID & Qwen-VAE & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\end{tabular}
\end{center}
\end{table}

Figure~\ref{fig:main_comparison} provides qualitative comparisons of the reconstructions across different metrics.

\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Comparison of VAE reconstructions on various metrics.}
\label{fig:main_comparison}
\end{figure}

\subsection{Ablation Study: Robustness to Distortions}

We conduct an ablation study to explore the robustness of VAEs to different types of input distortions such as noise and haze. Specifically, we test whether the models can reconstruct clean images from distorted inputs.

\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Reconstruction of clean images from distorted inputs (Ablation Study).}
\label{fig:ablation}
\end{figure}

\section{Discussion and Insights}

One key insight from our study is the potential use of VAEs as pre-processors for remote sensing data. In scenarios involving noise or haze, a zero-shot VAE pass can serve as a computationally efficient way to clean up the data before further analysis. This suggests that the priors learned from natural images can indeed provide a "robust" foundation for specialized domains, even without explicit fine-tuning. We observe that models like SDXL-VAE and FLUX-VAE maintain high structural integrity even in out-of-distribution RS samples.

\section{Conclusion}

In this work, we explored the robustness of natural English priors in VAEs for remote sensing. Our findings indicate that these models, when used zero-shot, can provide significant utility in data compression and pre-processing tasks. Future work could further explore the integration of domain-specific priors to enhance these capabilities.

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

% \newpage
% \appendix
% \section{Appendix}
% If you choose to include an appendix, please submit it as a separate PDF file.

\end{document}
