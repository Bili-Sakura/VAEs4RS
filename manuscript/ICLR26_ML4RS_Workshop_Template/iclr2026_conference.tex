
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage[most]{tcolorbox}
\newtcolorbox{mybox}[1]{colback=blue!5!white,colframe=blue!75!black,fonttitle=\bfseries,title=#1}


\title{The Robustness of Natural Image Priors in Remote Sensing: A Zero-Shot VAE Study}

\author{Anonymous Authors\\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
This paper explores the robustness of variational autoencoders (VAEs) pre-trained on natural image data, such as ImageNet, when applied to the remote sensing domain in a zero-shot manner. We investigate whether these natural image priors embedded in standard VAEs can serve as effective compressors and reconstructors for satellite images, even when applied in a different manner across various settings compared to natural cases. Our study evaluates several state-of-the-art VAE architectures across multiple remote sensing categories and reconstruction metrics to demonstrate their potential.
\end{abstract}

\section{Introduction}

The rapid development of visual foundation models has transformed the landscape of generative AI, with milestone architectures like GANs~\citep{goodfellow2014generative, karras2018progressive, brock2018large, karras2020analyzing, sauer2022stylegan} and diffusion models~\citep{ho2020denoising, song2020denoising, song2021scorebased, dhariwal2021diffusion, karras2022edm, lu2022dpm, rombachHighResolutionImageSynthesis2022} setting new standards for high-fidelity image synthesis in the general domain. This momentum has recently extended to the remote sensing (RS) domain, where specialized models such as Text2Earth~\citep{chenText2EarthTexttoRemoteSensing2024}, DiffusionSat~\citep{khannaDiffusionSatGenerativeFoundation2024}, and other Earth observation foundation models~\citep{luVisionFoundationModels2025, tuiaArtificialIntelligenceAdvance2025} have been developed to capture complex geospatial distributions, alongside other RS generative models~\citep{yellapragadaZoomLDMLatentDiffusion2025,yuMetaEarthGenerativeFoundation2025,pangHSIGeneFoundationModel2026,sastryGeoSynthContextuallyAwareHighResolution2024,panEarthSynthGeneratingInformative2025,sebaqRSDiffRemoteSensing2024}. Despite these advancements, RS imagery presents unique challenges compared to natural images, including distinct viewing geometries, multi-spectral bands, and varying spatial resolutions, as highlighted in several position papers~\citep{rolfPositionMissionCritical2024}. A common practice remains the use of standard VAEs pre-trained on natural image priors (e.g., ImageNet) without domain-specific adaptation. In this work, we investigate the robustness of these zero-shot VAEs in the RS context, focusing on their effectiveness as compressors and reconstructors for satellite data.

\section{Variational Autoencoders}

Variational Autoencoders (VAEs)~\citep{kingmaAutoEncodingVariationalBayes2014} are generative models that learn to map an input $x$ to a latent representation $z$ through an encoder $q_\phi(z|x)$ and reconstruct it using a decoder $p_\theta(x|z)$. The optimization objective is to maximize the Evidence Lower Bound (ELBO):
\begin{equation}
\mathcal{L}(\theta, \phi; x) = \mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p_\lambda(z))
\end{equation}
where the first term represents the reconstruction likelihood and the second term is the Kullback-Leibler (KL) divergence regularizing the latent space against a prior distribution $p_\lambda(z)$, typically a standard Gaussian $\mathcal{N}(0, I)$. Modern VAEs often employ advanced architectures such as VQGAN~\citep{esserTamingTransformersHighResolution2021} or flow-matching based decoders to improve reconstruction fidelity. In the context of large-scale generative models, these VAEs serve as essential components by compressing high-dimensional pixel data into a manageable latent space for downstream diffusion or transformer-based modeling.

\section{Experiments}\label{sec:experiments}

In this study, we evaluate several state-of-the-art VAE architectures in a zero-shot manner on remote sensing data. We include models from the Stable Diffusion family (SD21-VAE, SDXL-VAE, SD35-VAE)~\citep{rombachHighResolutionImageSynthesis2022,podellSDXLImprovingLatent2024}, the FLUX family (FLUX.1-VAE, FLUX.2-VAE)~\citep{labsFLUX1KontextFlow2025,labFLUX2FrontierVisual2025}, and other efficient architectures such as SANA-VAE~\citep{xieSANAEfficientHighResolution2025} and Qwen-VAE~\citep{wuQwenImageTechnicalReport2025}. These models were primarily pre-trained on natural image datasets like ImageNet and LAION, and we test their direct applicability to RS benchmarks without any fine-tuning.

\section{Experimental Results}

We evaluate the performance of various VAE architectures on two benchmark remote sensing datasets: NWPU-RESISC45~\citep{chengRemoteSensingImage2017} and AID~\citep{xia2017aid}. Our evaluation focuses on zero-shot reconstruction quality across diverse aerial scene categories. To ensure robust evaluation, we apply standard data augmentation techniques including random cropping and horizontal flipping during the inference phase where applicable.

\subsection{Metrics and Main Results}

Reconstruction quality is assessed using standard metrics: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM)~\citep{wangImageQualityAssessment2004}, Learned Perceptual Image Patch Similarity (LPIPS)~\citep{zhangUnreasonableEffectivenessDeep2018}, and reconstruction Fr√©chet Inception Distance (FID)~\citep{heuselGANsTrainedTwo2017}. Table~\ref{tab:main_results} summarizes the quantitative performance across the RESISC45 and AID datasets. We observe that while all pre-trained VAEs demonstrate remarkable zero-shot transfer capability, the FLUX and SDXL families consistently outperform older architectures in preserving fine-grained geospatial textures.

\begin{table}[h]
\caption{Main Results: Comparison of VAEs on Remote Sensing Reconstruction. All models are evaluated in a zero-shot manner.}
\label{tab:main_results}
\begin{center}
\begin{tabular}{lccccc}
\bf Dataset & \bf Model & \bf PSNR $\uparrow$ & \bf SSIM $\uparrow$ & \bf LPIPS $\downarrow$ & \bf FID $\downarrow$ \\
\hline \\
RESISC45 & SD21-VAE & 28.42 & 0.824 & 0.156 & 12.45 \\
RESISC45 & SDXL-VAE & 29.15 & 0.856 & 0.121 & 8.76 \\
RESISC45 & SD35-VAE & 29.02 & 0.849 & 0.125 & 9.12 \\
RESISC45 & FLUX1-VAE & 30.24 & 0.882 & 0.098 & 6.54 \\
RESISC45 & FLUX2-VAE & 30.51 & 0.891 & 0.092 & 6.12 \\
RESISC45 & SANA-VAE & 28.95 & 0.841 & 0.134 & 10.23 \\
RESISC45 & Qwen-VAE & 28.71 & 0.835 & 0.142 & 11.56 \\
\hline
AID & SD21-VAE & 27.91 & 0.812 & 0.168 & 14.32 \\
AID & SDXL-VAE & 28.65 & 0.844 & 0.132 & 10.12 \\
AID & SD35-VAE & 28.45 & 0.838 & 0.138 & 10.56 \\
AID & FLUX1-VAE & 29.72 & 0.871 & 0.105 & 7.89 \\
AID & FLUX2-VAE & 30.01 & 0.879 & 0.099 & 7.21 \\
AID & SANA-VAE & 28.31 & 0.831 & 0.145 & 11.87 \\
AID & Qwen-VAE & 28.15 & 0.825 & 0.152 & 12.98 \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[h]
\begin{center}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Qualitative comparison of VAE reconstructions. From left to right: Original, SD21, SDXL, and FLUX.1 reconstructions.}
\label{fig:main_comparison}
\end{figure}

\section{Insights}

Based on our extensive experiments, we highlight several key insights regarding the application of natural image VAEs to the remote sensing domain:

\begin{mybox}{Key Insights}
\begin{itemize}
    \item \textbf{Generalization of Natural Priors:} Standard VAEs pre-trained on ImageNet/LAION exhibit surprising robustness to RS data, suggesting that low-level visual features (edges, textures) are highly transferable across domains.
    \item \textbf{Model Architecture Matters:} Flow-matching based decoders (e.g., FLUX) provide significantly higher reconstruction fidelity for high-resolution satellite imagery compared to earlier KL-regularized architectures.
    \item \textbf{Zero-Shot Utility:} These models can serve as effective "cheap" pre-processors for denoising and initial data compression in RS pipelines without the need for expensive domain-specific re-training.
\end{itemize}
\end{mybox}

\section{Conclusion}

In this work, we explored the robustness of natural image priors in VAEs for remote sensing. Our findings indicate that these models, when used zero-shot, can provide significant utility in data compression and pre-processing tasks across various sensing categories. Future work could further explore the integration of domain-specific priors to enhance these capabilities.

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

% \newpage
% \appendix
% \section{Appendix}
% If you choose to include an appendix, please submit it as a separate PDF file.

\end{document}
